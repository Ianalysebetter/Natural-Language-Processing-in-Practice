{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cefca676",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "In moodle you will find the file ASoIaF.zip. It contains the five books of the “A song of ice\n",
    "and fire” series in a plain txt-format. Load all files into your console.\n",
    "\n",
    "We are interested in how the story and its themes develop over time. For this, we will train a\n",
    "topic model on each book and compare them. For Python-users, you will find a better function\n",
    "to show the top-words in utils.py.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4231418a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1607894, 1752880, 2273275, 1614153, 2277994]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simplifying the code for loading all files into the console\n",
    "\n",
    "# Paths to the text files for each book\n",
    "file_paths = [\n",
    "    \"ASoIaF/ASoIaF/001ssb.txt\",  # A Game of Thrones\n",
    "    \"ASoIaF/ASoIaF/002ssb.txt\",  # A Clash of Kings\n",
    "    \"ASoIaF/ASoIaF/003ssb.txt\",  # A Storm of Swords\n",
    "    \"ASoIaF/ASoIaF/004ssb.txt\",  # A Feast for Crows\n",
    "    \"ASoIaF/ASoIaF/005ssb.txt\"   # A Dance with Dragons\n",
    "]\n",
    "\n",
    "# Initialize a list to hold the content of each book\n",
    "books = []\n",
    "\n",
    "# Load the content of each book into the list\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "        books.append(file.read())\n",
    "\n",
    "# Lengths of each book to confirm they've been loaded\n",
    "[len(book) for book in books]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d335c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf20d33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 69, 82, 2, 3], 158)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To clean the texts and split them into chapters, we first need to identify common patterns\n",
    "# for chapter starts or any unwanted fragments to remove. Given the variability in formatting,\n",
    "# a common approach is to look for chapter headings, which might be consistently formatted.\n",
    "\n",
    "# Define a function to clean the text and split it into chapters\n",
    "def clean_and_split(text):\n",
    "    # Attempt to identify chapter headings and unwanted fragments\n",
    "    # Assuming chapters might start with \"CHAPTER\", \"Chapter\", or a similar keyword\n",
    "    # and are possibly followed by a chapter title or number\n",
    "    chapters = []\n",
    "    current_chapter = []\n",
    "    \n",
    "    for line in text.split('\\n'):\n",
    "        # Check if the line indicates the start of a new chapter\n",
    "        if line.startswith(\"CHAPTER\") or line.startswith(\"Chapter\") or line.startswith(\"PROLOGUE\"):\n",
    "            # If there's an existing chapter, save it and start a new one\n",
    "            if current_chapter:\n",
    "                chapters.append(\"\\n\".join(current_chapter))\n",
    "                current_chapter = [line]\n",
    "            else:\n",
    "                current_chapter.append(line)\n",
    "        else:\n",
    "            current_chapter.append(line)\n",
    "    \n",
    "    # Add the last chapter to the list, if it exists\n",
    "    if current_chapter:\n",
    "        chapters.append(\"\\n\".join(current_chapter))\n",
    "    \n",
    "    return chapters\n",
    "\n",
    "# Clean each book's text and split into chapters\n",
    "cleaned_books = [clean_and_split(book) for book in books]\n",
    "\n",
    "# Flatten the list of lists into a single list of chapters, preserving chronological order\n",
    "chapters = [chapter for book in cleaned_books for chapter in book]\n",
    "\n",
    "# Verify by displaying the number of chapters found in each book and total\n",
    "chapter_counts_per_book = [len(book) for book in cleaned_books]\n",
    "total_chapters = len(chapters)\n",
    "\n",
    "chapter_counts_per_book, total_chapters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faaf783",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cda97cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['game',\n",
       " 'throne',\n",
       " 'book',\n",
       " 'one',\n",
       " 'song',\n",
       " 'ice',\n",
       " 'fire',\n",
       " 'george',\n",
       " 'r',\n",
       " 'r',\n",
       " 'martin']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define a function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Preprocess all chapters\n",
    "preprocessed_chapters = [preprocess_text(chapter) for chapter in chapters]\n",
    "\n",
    "# Example: Display the first 100 tokens of the first preprocessed chapter\n",
    "preprocessed_chapters[0][:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb50f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2754bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Topics:\n",
      "(0, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"a\" + 0.000*\"to\" + 0.000*\"of\"')\n",
      "(1, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"to\" + 0.000*\"a\" + 0.000*\"of\"')\n",
      "(2, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"to\" + 0.000*\"of\" + 0.000*\"he\"')\n",
      "(3, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"a\" + 0.000*\"he\" + 0.000*\"his\"')\n",
      "(4, '0.060*\"the\" + 0.030*\"and\" + 0.022*\"to\" + 0.022*\"a\" + 0.020*\"of\"')\n",
      "(5, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"to\" + 0.000*\"his\" + 0.000*\"her\"')\n",
      "(6, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"a\" + 0.000*\"his\" + 0.000*\"her\"')\n",
      "(7, '0.000*\"the\" + 0.000*\"to\" + 0.000*\"and\" + 0.000*\"a\" + 0.000*\"of\"')\n",
      "(8, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"of\" + 0.000*\"a\" + 0.000*\"to\"')\n",
      "(9, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"a\" + 0.000*\"to\" + 0.000*\"his\"')\n",
      "\n",
      "\n",
      "Model 2 Topics:\n",
      "(0, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"to\" + 0.000*\"of\" + 0.000*\"a\"')\n",
      "(1, '0.000*\"the\" + 0.000*\"to\" + 0.000*\"he\" + 0.000*\"a\" + 0.000*\"and\"')\n",
      "(2, '0.000*\"the\" + 0.000*\"of\" + 0.000*\"to\" + 0.000*\"and\" + 0.000*\"his\"')\n",
      "(3, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"to\" + 0.000*\"he\" + 0.000*\"of\"')\n",
      "(4, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"a\" + 0.000*\"to\" + 0.000*\"of\"')\n",
      "(5, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"to\" + 0.000*\"a\" + 0.000*\"of\"')\n",
      "(6, '0.060*\"the\" + 0.030*\"and\" + 0.022*\"to\" + 0.022*\"a\" + 0.020*\"of\"')\n",
      "(7, '0.000*\"the\" + 0.000*\"to\" + 0.000*\"a\" + 0.000*\"and\" + 0.000*\"his\"')\n",
      "(8, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"a\" + 0.000*\"to\" + 0.000*\"he\"')\n",
      "(9, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"to\" + 0.000*\"a\" + 0.000*\"he\"')\n",
      "\n",
      "\n",
      "Model 3 Topics:\n",
      "(0, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"a\" + 0.000*\"to\" + 0.000*\"his\"')\n",
      "(1, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"a\" + 0.000*\"to\" + 0.000*\"his\"')\n",
      "(2, '0.060*\"the\" + 0.030*\"and\" + 0.022*\"to\" + 0.022*\"a\" + 0.020*\"of\"')\n",
      "(3, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"his\" + 0.000*\"he\" + 0.000*\"to\"')\n",
      "(4, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"a\" + 0.000*\"to\" + 0.000*\"of\"')\n",
      "(5, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"a\" + 0.000*\"to\" + 0.000*\"of\"')\n",
      "(6, '0.000*\"the\" + 0.000*\"a\" + 0.000*\"to\" + 0.000*\"of\" + 0.000*\"and\"')\n",
      "(7, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"to\" + 0.000*\"of\" + 0.000*\"a\"')\n",
      "(8, '0.000*\"the\" + 0.000*\"a\" + 0.000*\"and\" + 0.000*\"to\" + 0.000*\"his\"')\n",
      "(9, '0.000*\"the\" + 0.000*\"to\" + 0.000*\"a\" + 0.000*\"and\" + 0.000*\"he\"')\n",
      "\n",
      "\n",
      "Model 4 Topics:\n",
      "(0, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"of\" + 0.000*\"to\" + 0.000*\"his\"')\n",
      "(1, '0.000*\"the\" + 0.000*\"a\" + 0.000*\"to\" + 0.000*\"and\" + 0.000*\"of\"')\n",
      "(2, '0.000*\"the\" + 0.000*\"to\" + 0.000*\"a\" + 0.000*\"and\" + 0.000*\"of\"')\n",
      "(3, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"a\" + 0.000*\"to\" + 0.000*\"his\"')\n",
      "(4, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"to\" + 0.000*\"he\" + 0.000*\"of\"')\n",
      "(5, '0.060*\"the\" + 0.030*\"and\" + 0.022*\"to\" + 0.022*\"a\" + 0.020*\"of\"')\n",
      "(6, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"a\" + 0.000*\"to\" + 0.000*\"his\"')\n",
      "(7, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"a\" + 0.000*\"of\" + 0.000*\"he\"')\n",
      "(8, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"to\" + 0.000*\"his\" + 0.000*\"a\"')\n",
      "(9, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"to\" + 0.000*\"he\" + 0.000*\"a\"')\n",
      "\n",
      "\n",
      "Model 5 Topics:\n",
      "(0, '0.060*\"the\" + 0.030*\"and\" + 0.022*\"to\" + 0.022*\"a\" + 0.020*\"of\"')\n",
      "(1, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"he\" + 0.000*\"of\" + 0.000*\"to\"')\n",
      "(2, '0.000*\"the\" + 0.000*\"to\" + 0.000*\"a\" + 0.000*\"and\" + 0.000*\"he\"')\n",
      "(3, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"a\" + 0.000*\"to\" + 0.000*\"he\"')\n",
      "(4, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"a\" + 0.000*\"his\" + 0.000*\"to\"')\n",
      "(5, '0.001*\"the\" + 0.000*\"and\" + 0.000*\"of\" + 0.000*\"a\" + 0.000*\"to\"')\n",
      "(6, '0.000*\"the\" + 0.000*\"of\" + 0.000*\"and\" + 0.000*\"a\" + 0.000*\"his\"')\n",
      "(7, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"of\" + 0.000*\"to\" + 0.000*\"a\"')\n",
      "(8, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"to\" + 0.000*\"of\" + 0.000*\"a\"')\n",
      "(9, '0.000*\"the\" + 0.000*\"and\" + 0.000*\"a\" + 0.000*\"to\" + 0.000*\"he\"')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Before training LDA models, we need to ensure the text is properly preprocessed\n",
    "# Given the limitations in downloading NLTK data, we'll proceed with a simplified preprocessing\n",
    "\n",
    "# Let's focus on the very first book for this task\n",
    "first_book = books[0]\n",
    "\n",
    "# Simplified preprocessing for the first book (lowercasing, removing punctuation)\n",
    "def preprocess_text_simple(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenization\n",
    "    return tokens\n",
    "\n",
    "tokens_first_book = preprocess_text_simple(first_book)\n",
    "\n",
    "# For LDA, we need to create a document-term matrix\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "# Create a Dictionary from the tokens\n",
    "dictionary = corpora.Dictionary([tokens_first_book])\n",
    "\n",
    "# Convert to document-term matrix\n",
    "corpus = [dictionary.doc2bow(tokens_first_book)]\n",
    "\n",
    "# Set parameters\n",
    "num_topics = 10\n",
    "passes = 50\n",
    "\n",
    "# Train five LDA models with K=10 and 50 iterations on the first book\n",
    "ldas = [gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=passes) for _ in range(5)]\n",
    "\n",
    "# Display the topics from each model to compare\n",
    "for i, lda in enumerate(ldas, start=1):\n",
    "    print(f\"Model {i} Topics:\")\n",
    "    for topic in lda.print_topics(num_words=5):\n",
    "        print(topic)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb2f073b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Topics:\n",
      "(0, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"man\" + 0.000*\"men\"')\n",
      "(1, '0.000*\"ser\" + 0.000*\"lord\" + 0.000*\"jon\" + 0.000*\"hand\" + 0.000*\"eye\"')\n",
      "(2, '0.000*\"lord\" + 0.000*\"hand\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"eye\"')\n",
      "(3, '0.000*\"lord\" + 0.000*\"jon\" + 0.000*\"eye\" + 0.000*\"hand\" + 0.000*\"man\"')\n",
      "(4, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"jon\" + 0.000*\"man\" + 0.000*\"hand\"')\n",
      "(5, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"jon\" + 0.000*\"eye\"')\n",
      "(6, '0.014*\"lord\" + 0.009*\"ser\" + 0.008*\"hand\" + 0.008*\"man\" + 0.008*\"jon\"')\n",
      "(7, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"jon\" + 0.000*\"hand\" + 0.000*\"man\"')\n",
      "(8, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"hand\" + 0.000*\"jon\"')\n",
      "(9, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"tyrion\" + 0.000*\"hand\"')\n",
      "Model 2 Topics:\n",
      "(0, '0.000*\"lord\" + 0.000*\"hand\" + 0.000*\"jon\" + 0.000*\"man\" + 0.000*\"eye\"')\n",
      "(1, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"jon\" + 0.000*\"tyrion\"')\n",
      "(2, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"jon\" + 0.000*\"ser\" + 0.000*\"men\"')\n",
      "(3, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"hand\" + 0.000*\"tyrion\" + 0.000*\"eye\"')\n",
      "(4, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"page\" + 0.000*\"man\" + 0.000*\"hand\"')\n",
      "(5, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"jon\" + 0.000*\"man\" + 0.000*\"men\"')\n",
      "(6, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"jon\" + 0.000*\"ser\" + 0.000*\"hand\"')\n",
      "(7, '0.014*\"lord\" + 0.009*\"ser\" + 0.008*\"hand\" + 0.008*\"man\" + 0.008*\"jon\"')\n",
      "(8, '0.000*\"lord\" + 0.000*\"hand\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"jon\"')\n",
      "(9, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"jon\" + 0.000*\"ser\" + 0.000*\"eye\"')\n",
      "Model 3 Topics:\n",
      "(0, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"jon\" + 0.000*\"ser\" + 0.000*\"hand\"')\n",
      "(1, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"jon\" + 0.000*\"hand\" + 0.000*\"eye\"')\n",
      "(2, '0.000*\"lord\" + 0.000*\"eye\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"tyrion\"')\n",
      "(3, '0.014*\"lord\" + 0.009*\"ser\" + 0.008*\"hand\" + 0.008*\"man\" + 0.008*\"jon\"')\n",
      "(4, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"man\" + 0.000*\"jon\"')\n",
      "(5, '0.000*\"ser\" + 0.000*\"jon\" + 0.000*\"hand\" + 0.000*\"lord\" + 0.000*\"eye\"')\n",
      "(6, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"jon\" + 0.000*\"man\" + 0.000*\"tyrion\"')\n",
      "(7, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"hand\" + 0.000*\"jon\" + 0.000*\"ser\"')\n",
      "(8, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"tyrion\" + 0.000*\"hand\"')\n",
      "(9, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"jon\" + 0.000*\"hand\" + 0.000*\"man\"')\n",
      "Model 4 Topics:\n",
      "(0, '0.000*\"lord\" + 0.000*\"jon\" + 0.000*\"man\" + 0.000*\"ser\" + 0.000*\"eye\"')\n",
      "(1, '0.000*\"lord\" + 0.000*\"hand\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"eye\"')\n",
      "(2, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"jon\" + 0.000*\"ser\" + 0.000*\"hand\"')\n",
      "(3, '0.014*\"lord\" + 0.009*\"ser\" + 0.008*\"hand\" + 0.008*\"man\" + 0.008*\"jon\"')\n",
      "(4, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"jon\" + 0.000*\"hand\" + 0.000*\"men\"')\n",
      "(5, '0.000*\"lord\" + 0.000*\"jon\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"man\"')\n",
      "(6, '0.000*\"lord\" + 0.000*\"eye\" + 0.000*\"page\" + 0.000*\"ser\" + 0.000*\"jon\"')\n",
      "(7, '0.000*\"lord\" + 0.000*\"hand\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"tyrion\"')\n",
      "(8, '0.000*\"lord\" + 0.000*\"jon\" + 0.000*\"hand\" + 0.000*\"man\" + 0.000*\"ser\"')\n",
      "(9, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"man\" + 0.000*\"eye\"')\n",
      "Model 5 Topics:\n",
      "(0, '0.000*\"lord\" + 0.000*\"hand\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"eye\"')\n",
      "(1, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"jon\" + 0.000*\"man\"')\n",
      "(2, '0.003*\"screamer\" + 0.002*\"snatch\" + 0.002*\"trembling\" + 0.002*\"fugitive\" + 0.002*\"teacher\"')\n",
      "(3, '0.000*\"lord\" + 0.000*\"hand\" + 0.000*\"man\" + 0.000*\"jon\" + 0.000*\"page\"')\n",
      "(4, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"man\" + 0.000*\"tyrion\"')\n",
      "(5, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"jon\" + 0.000*\"eye\"')\n",
      "(6, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"men\" + 0.000*\"hand\"')\n",
      "(7, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"man\" + 0.000*\"tyrion\"')\n",
      "(8, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"jon\" + 0.000*\"hand\"')\n",
      "(9, '0.014*\"lord\" + 0.009*\"ser\" + 0.008*\"hand\" + 0.008*\"man\" + 0.008*\"jon\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define the advanced preprocessing function\n",
    "def preprocess_text_advanced(text):\n",
    "    # Extend the stop words list\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    tokens = [w for w in tokens if w not in stop_words]  # Remove stop words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]  # Lemmatization\n",
    "    # POS tagging and filtering nouns and adjectives\n",
    "    tokens = [word for word, tag in pos_tag(tokens) if tag.startswith('NN') or tag.startswith('JJ')]\n",
    "    return tokens\n",
    "\n",
    "# Load and preprocess the first book\n",
    "# Assuming 'first_book_text' contains the raw text of the first book\n",
    "first_book_text = books[0]  # Replace with the appropriate variable if different\n",
    "tokens = preprocess_text_advanced(first_book_text)\n",
    "\n",
    "# Create the Dictionary and Corpus\n",
    "dictionary = corpora.Dictionary([tokens])\n",
    "corpus = [dictionary.doc2bow(tokens)]\n",
    "\n",
    "# Train five LDA models\n",
    "ldas = [LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, passes=50) for _ in range(5)]\n",
    "\n",
    "# Compare the topics from these models\n",
    "for i, lda in enumerate(ldas, start=1):\n",
    "    print(f\"Model {i} Topics:\")\n",
    "    for topic in lda.show_topics(num_words=5):\n",
    "        print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6367ba76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Topics:\n",
      "(0, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"hand\" + 0.000*\"jon\"')\n",
      "(1, '0.000*\"ser\" + 0.000*\"lord\" + 0.000*\"man\" + 0.000*\"eye\" + 0.000*\"hand\"')\n",
      "(2, '0.000*\"lord\" + 0.000*\"jon\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"man\"')\n",
      "(3, '0.000*\"lord\" + 0.000*\"hand\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"jon\"')\n",
      "(4, '0.014*\"lord\" + 0.009*\"ser\" + 0.008*\"hand\" + 0.008*\"man\" + 0.008*\"jon\"')\n",
      "(5, '0.000*\"lord\" + 0.000*\"hand\" + 0.000*\"man\" + 0.000*\"men\" + 0.000*\"jon\"')\n",
      "(6, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"jon\" + 0.000*\"tyrion\"')\n",
      "(7, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"men\" + 0.000*\"hand\" + 0.000*\"man\"')\n",
      "(8, '0.000*\"lord\" + 0.000*\"hand\" + 0.000*\"man\" + 0.000*\"ser\" + 0.000*\"tyrion\"')\n",
      "(9, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"hand\" + 0.000*\"jon\"')\n",
      "Model 2 Topics:\n",
      "(0, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"jon\" + 0.000*\"hand\"')\n",
      "(1, '0.000*\"lord\" + 0.000*\"hand\" + 0.000*\"jon\" + 0.000*\"tyrion\" + 0.000*\"man\"')\n",
      "(2, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"jon\" + 0.000*\"page\"')\n",
      "(3, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"jon\" + 0.000*\"hand\" + 0.000*\"man\"')\n",
      "(4, '0.014*\"lord\" + 0.009*\"ser\" + 0.008*\"hand\" + 0.008*\"man\" + 0.008*\"jon\"')\n",
      "(5, '0.000*\"lord\" + 0.000*\"men\" + 0.000*\"jon\" + 0.000*\"ser\" + 0.000*\"man\"')\n",
      "(6, '0.000*\"lord\" + 0.000*\"hand\" + 0.000*\"ser\" + 0.000*\"jon\" + 0.000*\"man\"')\n",
      "(7, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"jon\" + 0.000*\"hand\" + 0.000*\"man\"')\n",
      "(8, '0.000*\"lord\" + 0.000*\"hand\" + 0.000*\"man\" + 0.000*\"ser\" + 0.000*\"jon\"')\n",
      "(9, '0.000*\"lord\" + 0.000*\"hand\" + 0.000*\"eye\" + 0.000*\"ser\" + 0.000*\"man\"')\n",
      "Model 3 Topics:\n",
      "(0, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"hand\" + 0.000*\"jon\"')\n",
      "(1, '0.014*\"lord\" + 0.009*\"ser\" + 0.008*\"hand\" + 0.008*\"man\" + 0.008*\"jon\"')\n",
      "(2, '0.000*\"lord\" + 0.000*\"jon\" + 0.000*\"hand\" + 0.000*\"ser\" + 0.000*\"man\"')\n",
      "(3, '0.000*\"lord\" + 0.000*\"hand\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"jon\"')\n",
      "(4, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"tyrion\" + 0.000*\"jon\" + 0.000*\"page\"')\n",
      "(5, '0.000*\"ser\" + 0.000*\"lord\" + 0.000*\"man\" + 0.000*\"hand\" + 0.000*\"men\"')\n",
      "(6, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"man\" + 0.000*\"jon\"')\n",
      "(7, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"hand\" + 0.000*\"eye\"')\n",
      "(8, '0.000*\"lord\" + 0.000*\"hand\" + 0.000*\"man\" + 0.000*\"ser\" + 0.000*\"jon\"')\n",
      "(9, '0.000*\"lord\" + 0.000*\"hand\" + 0.000*\"ser\" + 0.000*\"tyrion\" + 0.000*\"man\"')\n",
      "Model 4 Topics:\n",
      "(0, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"jon\" + 0.000*\"man\" + 0.000*\"hand\"')\n",
      "(1, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"hand\" + 0.000*\"bran\"')\n",
      "(2, '0.000*\"lord\" + 0.000*\"jon\" + 0.000*\"man\" + 0.000*\"hand\" + 0.000*\"ser\"')\n",
      "(3, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"man\" + 0.000*\"jon\"')\n",
      "(4, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"jon\" + 0.000*\"men\" + 0.000*\"tyrion\"')\n",
      "(5, '0.000*\"lord\" + 0.000*\"jon\" + 0.000*\"hand\" + 0.000*\"ser\" + 0.000*\"man\"')\n",
      "(6, '0.014*\"lord\" + 0.009*\"ser\" + 0.008*\"hand\" + 0.008*\"man\" + 0.008*\"jon\"')\n",
      "(7, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"eye\" + 0.000*\"tyrion\"')\n",
      "(8, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"page\" + 0.000*\"jon\"')\n",
      "(9, '0.000*\"lord\" + 0.000*\"eye\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"men\"')\n",
      "Model 5 Topics:\n",
      "(0, '0.000*\"lord\" + 0.000*\"hand\" + 0.000*\"man\" + 0.000*\"ser\" + 0.000*\"page\"')\n",
      "(1, '0.000*\"lord\" + 0.000*\"hand\" + 0.000*\"ser\" + 0.000*\"jon\" + 0.000*\"eye\"')\n",
      "(2, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"jon\" + 0.000*\"hand\" + 0.000*\"eye\"')\n",
      "(3, '0.000*\"hand\" + 0.000*\"lord\" + 0.000*\"man\" + 0.000*\"ser\" + 0.000*\"men\"')\n",
      "(4, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"eye\"')\n",
      "(5, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"jon\" + 0.000*\"man\" + 0.000*\"hand\"')\n",
      "(6, '0.014*\"lord\" + 0.009*\"ser\" + 0.008*\"hand\" + 0.008*\"man\" + 0.008*\"jon\"')\n",
      "(7, '0.000*\"lord\" + 0.000*\"jon\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"tyrion\"')\n",
      "(8, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"hand\" + 0.000*\"jon\" + 0.000*\"ser\"')\n",
      "(9, '0.000*\"lord\" + 0.000*\"hand\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"jon\"')\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Assuming the text has been preprocessed with advanced techniques\n",
    "\n",
    "# Tokenize, stop word removal, lemmatization, and POS filtering for the first book\n",
    "tokens = preprocess_text_advanced(first_book_text)  # Use the advanced preprocessing function provided\n",
    "\n",
    "# Create a Dictionary and Corpus for LDA\n",
    "dictionary = Dictionary([tokens])\n",
    "corpus = [dictionary.doc2bow(text) for text in [tokens]]\n",
    "\n",
    "# Model parameters\n",
    "num_topics = 10\n",
    "passes = 50\n",
    "alpha = 'auto'  # Let the model automatically learn the optimal alpha\n",
    "eta = 'auto'    # Corrected parameter for topic-word distribution\n",
    "\n",
    "# Train five LDA models with the adjusted parameters\n",
    "ldas = [LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=passes, alpha=alpha, eta=eta) for _ in range(5)]\n",
    "\n",
    "# Compare the topics\n",
    "for i, lda in enumerate(ldas, start=1):\n",
    "    print(f\"Model {i} Topics:\")\n",
    "    for topic in lda.show_topics(num_words=5):\n",
    "        print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f955cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1ad605c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book 1 Topics:\n",
      "(0, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"jon\"')\n",
      "(1, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"man\" + 0.000*\"page\"')\n",
      "(2, '0.014*\"lord\" + 0.009*\"ser\" + 0.008*\"hand\" + 0.008*\"man\" + 0.008*\"jon\"')\n",
      "(3, '0.000*\"lord\" + 0.000*\"jon\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"hand\"')\n",
      "(4, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"hand\" + 0.000*\"jon\" + 0.000*\"eye\"')\n",
      "(5, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"men\" + 0.000*\"man\"')\n",
      "(6, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"jon\" + 0.000*\"man\" + 0.000*\"hand\"')\n",
      "(7, '0.000*\"lord\" + 0.000*\"jon\" + 0.000*\"hand\" + 0.000*\"man\" + 0.000*\"ser\"')\n",
      "(8, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"jon\" + 0.000*\"hand\" + 0.000*\"ser\"')\n",
      "(9, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"hand\" + 0.000*\"ser\" + 0.000*\"jon\"')\n",
      "\n",
      "\n",
      "Book 2 Topics:\n",
      "(0, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"men\"')\n",
      "(1, '0.014*\"lord\" + 0.009*\"man\" + 0.008*\"ser\" + 0.007*\"men\" + 0.007*\"hand\"')\n",
      "(2, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"men\" + 0.000*\"hand\"')\n",
      "(3, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"men\" + 0.000*\"man\" + 0.000*\"hand\"')\n",
      "(4, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"men\" + 0.000*\"tyrion\" + 0.000*\"page\"')\n",
      "(5, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"men\" + 0.000*\"ser\" + 0.000*\"hand\"')\n",
      "(6, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"men\" + 0.000*\"man\" + 0.000*\"brother\"')\n",
      "(7, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"man\" + 0.000*\"men\"')\n",
      "(8, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"hand\" + 0.000*\"men\"')\n",
      "(9, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"tyrion\" + 0.000*\"time\"')\n",
      "\n",
      "\n",
      "Book 3 Topics:\n",
      "(0, '0.000*\"lord\" + 0.000*\"men\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"man\"')\n",
      "(1, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"time\"')\n",
      "(2, '0.013*\"lord\" + 0.008*\"ser\" + 0.008*\"man\" + 0.007*\"hand\" + 0.006*\"men\"')\n",
      "(3, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"page\"')\n",
      "(4, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"hand\" + 0.000*\"page\"')\n",
      "(5, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"old\"')\n",
      "(6, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"men\" + 0.000*\"hand\"')\n",
      "(7, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"hand\" + 0.000*\"men\"')\n",
      "(8, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"hand\" + 0.000*\"man\" + 0.000*\"jon\"')\n",
      "(9, '0.000*\"lord\" + 0.000*\"hand\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"men\"')\n",
      "\n",
      "\n",
      "Book 4 Topics:\n",
      "(0, '0.000*\"ser\" + 0.000*\"man\" + 0.000*\"\\x93i\" + 0.000*\"lord\" + 0.000*\"jaime\"')\n",
      "(1, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"\\x93i\" + 0.000*\"hand\" + 0.000*\"jaime\"')\n",
      "(2, '0.000*\"ser\" + 0.000*\"lord\" + 0.000*\"man\" + 0.000*\"\\x93i\" + 0.000*\"hand\"')\n",
      "(3, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"\\x93i\" + 0.000*\"ser\" + 0.000*\"lady\"')\n",
      "(4, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"\\x93i\" + 0.000*\"man\" + 0.000*\"hand\"')\n",
      "(5, '0.000*\"lord\" + 0.000*\"\\x93i\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"cersei\"')\n",
      "(6, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"\\x93i\" + 0.000*\"lady\"')\n",
      "(7, '0.000*\"\\x93i\" + 0.000*\"lord\" + 0.000*\"man\" + 0.000*\"ser\" + 0.000*\"hand\"')\n",
      "(8, '0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"hand\" + 0.000*\"\\x93i\"')\n",
      "(9, '0.011*\"lord\" + 0.008*\"\\x93i\" + 0.008*\"ser\" + 0.008*\"man\" + 0.006*\"hand\"')\n",
      "\n",
      "\n",
      "Book 5 Topics:\n",
      "(0, '0.008*\"lord\" + 0.007*\"men\" + 0.007*\"man\" + 0.006*\"\\x93i\" + 0.006*\"jon\"')\n",
      "(1, '0.000*\"lord\" + 0.000*\"man\" + 0.000*\"men\" + 0.000*\"hand\" + 0.000*\"\\x93i\"')\n",
      "(2, '0.000*\"lord\" + 0.000*\"jon\" + 0.000*\"man\" + 0.000*\"\\x93i\" + 0.000*\"hand\"')\n",
      "(3, '0.000*\"lord\" + 0.000*\"men\" + 0.000*\"\\x93i\" + 0.000*\"eye\" + 0.000*\"jon\"')\n",
      "(4, '0.000*\"man\" + 0.000*\"lord\" + 0.000*\"ser\" + 0.000*\"men\" + 0.000*\"black\"')\n",
      "(5, '0.000*\"\\x93i\" + 0.000*\"men\" + 0.000*\"lord\" + 0.000*\"jon\" + 0.000*\"man\"')\n",
      "(6, '0.000*\"lord\" + 0.000*\"jon\" + 0.000*\"hand\" + 0.000*\"men\" + 0.000*\"eye\"')\n",
      "(7, '0.000*\"lord\" + 0.000*\"men\" + 0.000*\"ser\" + 0.000*\"man\" + 0.000*\"old\"')\n",
      "(8, '0.000*\"lord\" + 0.000*\"men\" + 0.000*\"\\x93i\" + 0.000*\"hand\" + 0.000*\"wall\"')\n",
      "(9, '0.000*\"lord\" + 0.000*\"men\" + 0.000*\"jon\" + 0.000*\"man\" + 0.000*\"hand\"')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure the necessary NLTK data is available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Assuming each book's text is loaded into a list 'books' where each element is the text of one book\n",
    "\n",
    "# Advanced preprocessing function (as defined earlier)\n",
    "def preprocess_text_advanced(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    tokens = [word for word, tag in pos_tag(tokens) if tag.startswith('NN') or tag.startswith('JJ')]\n",
    "    return tokens\n",
    "\n",
    "# Train an LDA model for each book\n",
    "lda_models = []\n",
    "for book_text in books:\n",
    "    tokens = preprocess_text_advanced(book_text)\n",
    "    dictionary = Dictionary([tokens])\n",
    "    corpus = [dictionary.doc2bow(tokens)]\n",
    "    lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, passes=50)\n",
    "    lda_models.append(lda)\n",
    "\n",
    "# To compare the models, examine the topics generated for each book\n",
    "for i, lda_model in enumerate(lda_models, start=1):\n",
    "    print(f\"Book {i} Topics:\")\n",
    "    for topic in lda_model.show_topics(num_words=5):\n",
    "        print(topic)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497fa67d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
