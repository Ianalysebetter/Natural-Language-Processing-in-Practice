{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85c45af4",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "In Moodle you will find the file trump.xml, containing Speeches of Donald Tump’s speeches\n",
    "during the campaign rallies of his 2016 presidential election.\n",
    "\n",
    "The file also contains meta information about the place and date of the speech. We are however\n",
    "only interested in the speeches themselves. Read the xml file into your console as if it were a\n",
    "simple text-file and then use Regex to filter out the speeches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "340d3e2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Step 1: Extracting Speeches\n",
    "# Assuming 'xml_content' contains the text content of the XML file\n",
    "with open('trump.xml', 'r', encoding='utf-8') as file:\n",
    "    xml_content = file.read()\n",
    "\n",
    "speeches = re.findall(r'<Speech>(.*?)</Speech>', xml_content, re.DOTALL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73930c0",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "Apply elementary tokenization steps. That is, within each speech\n",
    "* Remove punctuation, numbers and special characters\n",
    "* Turn all letters into lower case\n",
    "* Tokenize the text into individual words\n",
    "\n",
    "The result should be a list of lists (list of vectors for R). Each inner list represents a speech as\n",
    "a list of words.\n",
    "Count how often each word occurs in this text corpus and display the 10 most common words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1038517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 14144),\n",
       " ('and', 11282),\n",
       " ('to', 9333),\n",
       " ('a', 7888),\n",
       " ('you', 7694),\n",
       " ('i', 7623),\n",
       " ('of', 6818),\n",
       " ('we', 6526),\n",
       " ('it', 5584),\n",
       " ('they', 5520)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Function to clean and tokenize each speech\n",
    "def clean_and_tokenize(speech):\n",
    "    # Remove punctuation and numbers, and turn all letters into lower case\n",
    "    cleaned_text = re.sub(r'[' + string.punctuation + string.digits + ']', '', speech).lower()\n",
    "    # Tokenize the text into individual words\n",
    "    tokens = cleaned_text.split()\n",
    "    return tokens\n",
    "\n",
    "# Applying the cleaning and tokenization function to each speech\n",
    "tokenized_speeches = [clean_and_tokenize(speech) for speech in speeches]\n",
    "\n",
    "# Flattening the list of lists to count word frequencies across all speeches\n",
    "all_words = [word for speech in tokenized_speeches for word in speech]\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Displaying the 10 most common words\n",
    "most_common_words = word_counts.most_common(10)\n",
    "most_common_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b11d22",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "Use each one automated word stemming- and lemmatization method for your programming\n",
    "language. Apply them to the corpus resulting from task 2 and compare the resulting texts\n",
    "when applying each. Which of the two approaches would you prefer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f3d747a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(450,\n",
       " 479,\n",
       " ['thank',\n",
       "  'you',\n",
       "  'thank',\n",
       "  'you',\n",
       "  'thank',\n",
       "  'you',\n",
       "  'to',\n",
       "  'vice',\n",
       "  'presid',\n",
       "  'penc'],\n",
       " ['thank',\n",
       "  'you',\n",
       "  'thank',\n",
       "  'you',\n",
       "  'thank',\n",
       "  'you',\n",
       "  'to',\n",
       "  'vice',\n",
       "  'president',\n",
       "  'penny'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Downloading required NLTK resources if not already present\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initializing stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Applying stemming and lemmatization to a sample from the corpus\n",
    "# To manage the computation, we'll use a subset of the corpus for demonstration\n",
    "sample_speech_words = tokenized_speeches[0][:2000]  # Using the first 200 words of the first speech\n",
    "\n",
    "# Stemming\n",
    "stemmed_words = [stemmer.stem(word) for word in sample_speech_words]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in sample_speech_words]\n",
    "\n",
    "# Counting the unique words after stemming and lemmatization\n",
    "stemmed_unique_words = len(set(stemmed_words))\n",
    "lemmatized_unique_words = len(set(lemmatized_words))\n",
    "\n",
    "stemmed_unique_words, lemmatized_unique_words, stemmed_words[:10], lemmatized_words[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527644df",
   "metadata": {},
   "source": [
    "###  Which of the two approaches would you prefer? \n",
    "\n",
    "I would prefer lemmatization. The output indicates that lemmatization retains more accurate and meaningful forms of words (\"president\" instead of \"presid\"), which suggests it is better suited for applications requiring a high level of understanding and preservation of the original text's meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1ad068",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "\n",
    "Use your ”best” corpus from task 3 and apply stop word removal. That is, remove every word\n",
    "from a stop word list from your text. Beware that you have to apply the same pre-processing\n",
    "of your text to your stop words, such as removing the apostrophe from ”don’t”.\n",
    "Compare the most common words with the results from task 2. What do you notice?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7195c9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('going', 2319),\n",
       " ('said', 2217),\n",
       " ('people', 2144),\n",
       " ('know', 2039),\n",
       " ('great', 2005),\n",
       " ('dont', 1915),\n",
       " ('right', 1665),\n",
       " ('like', 1504),\n",
       " ('thats', 1490),\n",
       " ('want', 1489)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we've decided that lemmatization provides the \"best\" corpus for our purposes,\n",
    "# we will apply stop word removal to the lemmatized text.\n",
    "# First, we need to obtain a list of English stop words.\n",
    "\n",
    "# NLTK provides a comprehensive list of English stop words.\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Getting English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Pre-processing the stop words similar to our corpus pre-processing\n",
    "# This involves removing punctuation (if any) and converting to lower case\n",
    "# Since our lemmatized text is already in lower case and without punctuation, we can use the stop words as is\n",
    "\n",
    "# Removing stop words from the lemmatized corpus\n",
    "lemmatized_text_without_stopwords = [[word for word in speech if word not in stop_words]\n",
    "                                     for speech in tokenized_speeches]\n",
    "\n",
    "# Flattening the list of lists to count word frequencies\n",
    "all_words_without_stopwords = [word for speech in lemmatized_text_without_stopwords for word in speech]\n",
    "word_counts_without_stopwords = Counter(all_words_without_stopwords)\n",
    "\n",
    "# Displaying the 10 most common words after stop word removal\n",
    "most_common_words_without_stopwords = word_counts_without_stopwords.most_common(10)\n",
    "most_common_words_without_stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7276979e",
   "metadata": {},
   "source": [
    "Result from Task 2: \n",
    "\n",
    "[('the', 14144),\n",
    " ('and', 11282),\n",
    " ('to', 9333),\n",
    " ('a', 7888),\n",
    " ('you', 7694),\n",
    " ('i', 7623),\n",
    " ('of', 6818),\n",
    " ('we', 6526),\n",
    " ('it', 5584),\n",
    " ('they', 5520)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782101c",
   "metadata": {},
   "source": [
    "The comparison between the results of Task 2 and Task 4 demonstrates the importance of stop word removal in text preprocessing. It significantly enhances the focus on meaningful content, providing a clearer and more insightful basis for further analysis of the text corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40a5a79",
   "metadata": {},
   "source": [
    "### Thank you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24c7953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
